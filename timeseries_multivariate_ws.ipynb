{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timeseries-multivariate-ws.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNhBEFA6Vu2RwzV4pgbNjmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faizuddin/workshop/blob/main/timeseries_multivariate_ws.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwMwKNKkAIGY"
      },
      "source": [
        "# Multivariate RNNs\n",
        "\n",
        "Neural networks like Long Short-Term Memory (LSTM) recurrent neural networks are able to almost seamlessly model problems with multiple input variables.\n",
        "\n",
        "This is a great benefit in time series forecasting, where classical linear methods can be difficult to adapt to multivariate or multiple input forecasting problems.\n",
        "\n",
        "## Air Quality Forecasting\n",
        "This is a dataset that reports on the weather and the level of pollution each hour for five years at the US embassy in Beijing, China provided by [UCI Machine Learning Repository](\"https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data\").\n",
        "\n",
        "The data includes the date-time, the pollution called PM2.5 concentration, and the weather information including dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. The complete feature list in the raw data is as follows:\n",
        "\n",
        "1. `No`: row number\n",
        "2. `year`: year of data in this row\n",
        "3. `month`: month of data in this row\n",
        "4. `day`: day of data in this row\n",
        "5. `hour`: hour of data in this row\n",
        "6. `pm2.5`: PM2.5 concentration\n",
        "7. `DEWP`: Dew Point\n",
        "8. `TEMP`: Temperature\n",
        "9. `PRES`: Pressure\n",
        "10. `cbwd`: Combined wind direction\n",
        "11. `Iws`: Cumulated wind speed\n",
        "12. `Is`: Cumulated hours of snow\n",
        "13. `Ir`: Cumulated hours of rain\n",
        "\n",
        "### Forecasting Task\n",
        "Given the weather conditions and pollution for prior hours, we forecast the pollution at the next hour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJLgeGJXBdCE"
      },
      "source": [
        "## Basic Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwZiGDdeBxej"
      },
      "source": [
        "# Download dataset from UCI ML repository\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU8_oPq0CRQI"
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# load data\n",
        "def parse(x):\n",
        "\treturn datetime.strptime(x, '%Y %m %d %H')\n",
        "\n",
        "dataset = pd.read_csv('PRSA_data_2010.1.1-2014.12.31.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n",
        "\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-2x6ZoCCKod"
      },
      "source": [
        "The first step is to consolidate the date-time information into a single date-time so that we can use it as an index in Pandas.\n",
        "\n",
        "A quick check reveals NA values for pm2.5 for the first 24 hours. We will, therefore, need to remove the first row of data. There are also a few scattered “NA” values later in the dataset; we can mark them with 0 values for now.\n",
        "\n",
        "The script below loads the raw dataset and parses the date-time information as the Pandas DataFrame index. The “No” column is dropped and then clearer names are specified for each column. Finally, the NA values are replaced with “0” values and the first 24 hours are removed.\n",
        "\n",
        "The “No” column is dropped and then clearer names are specified for each column. Finally, the NA values are replaced with “0” values and the first 24 hours are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3WX7h9N_-wo"
      },
      "source": [
        "dataset.drop('No', axis=1, inplace=True)\n",
        "\n",
        "# manually specify column names\n",
        "dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
        "dataset.index.name = 'date'\n",
        "\n",
        "# mark all NA values with 0\n",
        "dataset['pollution'].fillna(0, inplace=True)\n",
        "\n",
        "# drop the first 24 hours\n",
        "dataset = dataset[24:]\n",
        "\n",
        "# summarize first 5 rows\n",
        "print(dataset.head(5))\n",
        "\n",
        "# save to file\n",
        "dataset.to_csv('pollution.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njh29j_ICyJI"
      },
      "source": [
        "Now that we have the data in an easy-to-use form, we can create a quick plot of each series and see what we have.\n",
        "\n",
        "The code below loads the new “pollution.csv” file and plots each series as a separate subplot, except wind speed dir, which is categorical. Running the example creates a plot with 7 subplots showing the 5 years of data for each variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FckbfXjkCAMQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load dataset\n",
        "dataset = pd.read_csv('pollution.csv', header=0, index_col=0)\n",
        "values = dataset.values\n",
        "\n",
        "# specify columns to plot\n",
        "groups = [0, 1, 2, 3, 5, 6, 7]\n",
        "i = 1\n",
        "\n",
        "# plot each column\n",
        "plt.figure(figsize=(10,10))\n",
        "for group in groups:\n",
        "\tplt.subplot(len(groups), 1, i)\n",
        "\tplt.plot(values[:, group])\n",
        "\tplt.title(dataset.columns[group], y=0.5, loc='right')\n",
        "\ti += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsbhQfxmDbgH"
      },
      "source": [
        "## LSTM Data Preparation\n",
        "\n",
        "This involves framing the dataset as a supervised learning problem and normalizing the input variables.\n",
        "\n",
        "We will frame the supervised learning problem as predicting the pollution at the current hour (`t`) given the pollution measurement and weather conditions at the prior time step.\n",
        "\n",
        "This formulation is straightforward and just for this demonstration. Some alternate formulations you could explore include:\n",
        "\n",
        "1. Predict the pollution for the next hour based on the weather conditions and pollution over the last 24 hours.\n",
        "2. Predict the pollution for the next hour as above and given the “expected” weather conditions for the next hour.\n",
        "\n",
        "We can transform the dataset using the `series_to_supervised()` function developed previously in the univariate LTSM example.\n",
        "\n",
        "The preparation steps:\n",
        "\n",
        "1. Load “pollution.csv” dataset.\n",
        "2. Encode the wind direction (`wnd_dir`) (integer encoded). This could further be one-hot encoded in the future if you are interested in exploring it.\n",
        "3. Normalise all features\n",
        "4. Remove weather variables for the hour to be predicted (`t`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xubFUhisC-wL"
      },
      "source": [
        "# prepare data for lstm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t\n",
        "  # put it all together\n",
        "\tagg = pd.concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t\n",
        "  # drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg\n",
        "\n",
        "# load dataset\n",
        "dataset = pd.read_csv('pollution.csv', header=0, index_col=0)\n",
        "values = dataset.values\n",
        "\n",
        "# integer encode direction\n",
        "encoder = LabelEncoder()\n",
        "values[:,4] = encoder.fit_transform(values[:,4])\n",
        "\n",
        "# ensure all data is float\n",
        "values = values.astype('float32')\n",
        "\n",
        "# normalize features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(values)\n",
        "\n",
        "# frame as supervised learning\n",
        "reframed = series_to_supervised(scaled, 1, 1)\n",
        "\n",
        "# drop columns we don't want to predict\n",
        "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
        "\n",
        "print(reframed.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RrahhxHFDEP"
      },
      "source": [
        "##Define and Fit Model\n",
        "\n",
        "First, we must split the prepared dataset into train and test sets. To speed up the training of the model for this demonstration, we will only fit the model on the first year of data, then evaluate it on the remaining 4 years of data. If you have time, consider exploring the inverted version of this test harness.\n",
        "\n",
        "The example below splits the dataset into train and test sets, then splits the train and test sets into input and output variables. Finally, the inputs (`X`) are reshaped into the 3D format expected by LSTMs, namely `[samples, timesteps, features]`.\n",
        "\n",
        "```\n",
        "...\n",
        "# split into train and test sets\n",
        "values = reframed.values\n",
        "n_train_hours = 365 * 24\n",
        "train = values[:n_train_hours, :]\n",
        "test = values[n_train_hours:, :]\n",
        "\n",
        "# split into input and outputs\n",
        "train_X, train_y = train[:, :-1], train[:, -1]\n",
        "test_X, test_y = test[:, :-1], test[:, -1]\n",
        "\n",
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "```\n",
        "We will define the LSTM with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting pollution. The input shape will be 1 time step with 8 features.\n",
        "\n",
        "We will use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.\n",
        "\n",
        "The model will be fit for 50 training epochs with a batch size of 72. Remember that the internal state of the LSTM in Keras is reset at the end of each batch, so an internal state that is a function of a number of days may be helpful (try testing this).\n",
        "\n",
        "Finally, we keep track of both the training and test loss during training by setting the validation_data argument in the fit() function. At the end of the run both the training and test loss are plotted.\n",
        "\n",
        "```\n",
        "...\n",
        "# design network\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "# fit network\n",
        "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "\n",
        "# plot history\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIrET9G8FonE"
      },
      "source": [
        "##Evaluate Model\n",
        "After the model is fit, we can forecast for the entire test dataset.\n",
        "\n",
        "We combine the forecast with the test dataset and invert the scaling. We also invert scaling on the test dataset with the expected pollution numbers.\n",
        "\n",
        "With forecasts and actual values in their original scale, we can then calculate an error score for the model. In this case, we calculate the Root Mean Squared Error (RMSE) that gives error in the same units as the variable itself.\n",
        "\n",
        "```\n",
        "...\n",
        "# make a prediction\n",
        "yhat = model.predict(test_X)\n",
        "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "\n",
        "# invert scaling for forecast\n",
        "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat = inv_yhat[:,0]\n",
        "\n",
        "# invert scaling for actual\n",
        "test_y = test_y.reshape((len(test_y), 1))\n",
        "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y = inv_y[:,0]\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxb4T75uF1A0"
      },
      "source": [
        "## Complete Multivariate LSTM Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kigP4sliE3CK"
      },
      "source": [
        "# load dataset\n",
        "dataset = pd.read_csv('pollution.csv', header=0, index_col=0)\n",
        "values = dataset.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vroP4qStGIR4"
      },
      "source": [
        "### Encode features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlDS8w25GLcy"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# integer encode direction\n",
        "encoder = LabelEncoder()\n",
        "values[:,4] = encoder.fit_transform(values[:,4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAR1mTQrGVvw"
      },
      "source": [
        "# ensure all data is float\n",
        "values = values.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGI2FDJcGbRu"
      },
      "source": [
        "### Normalise features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGwgb1XzGf0Q"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# normalize features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAmnJCcfHDLf"
      },
      "source": [
        "### Timeseries to supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EC8vgjwGkEU"
      },
      "source": [
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t\n",
        "  # input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t\n",
        "  # forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t\n",
        "  # put it all together\n",
        "\tagg = pd.concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t\n",
        "  # drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg\n",
        "\n",
        "# frame as supervised learning\n",
        "reframed = series_to_supervised(scaled, 1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmiYmbLoHSwU"
      },
      "source": [
        "### Drop the variables we are not going to predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMeU1Fm4Gszf"
      },
      "source": [
        "# drop columns we don't want to predict\n",
        "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
        "print(reframed.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1by2tR-NHhuO"
      },
      "source": [
        "### Training-test sets split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCnwlqAGHlGu"
      },
      "source": [
        "# split into train and test sets\n",
        "values = reframed.values\n",
        "n_train_hours = 365 * 24\n",
        "train = values[:n_train_hours, :]\n",
        "test = values[n_train_hours:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HbjjRuGHmGV"
      },
      "source": [
        "# split into input and outputs\n",
        "train_X, train_y = train[:, :-1], train[:, -1]\n",
        "test_X, test_y = test[:, :-1], test[:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD-fZ6cpHwHB"
      },
      "source": [
        "### Re-shape into LSTM input (3D)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU03iWKLH1N3"
      },
      "source": [
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6SAxKKYIBb4"
      },
      "source": [
        "### Configure RNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HPpN0vfH3zp"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# design network\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRC6j3a7IR6Z"
      },
      "source": [
        "### Train network\n",
        "\n",
        "This will train the network and create a plot showing the train and test loss during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k0s-igFIJqm"
      },
      "source": [
        "# fit network\n",
        "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "# plot history\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QqWuDxCIX2z"
      },
      "source": [
        "### Make prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoGC3x8ZIr2N"
      },
      "source": [
        "# make a prediction\n",
        "yhat = model.predict(test_X)\n",
        "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWzJEpY8IuTi"
      },
      "source": [
        "### Invert (scaling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI3L9NxXI0sA"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# invert scaling for forecast\n",
        "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat = inv_yhat[:,0]\n",
        "\n",
        "# invert scaling for actual\n",
        "test_y = test_y.reshape((len(test_y), 1))\n",
        "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y = inv_y[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O60r7DdiI4Qu"
      },
      "source": [
        "### Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rlAcxXrI7tO"
      },
      "source": [
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}